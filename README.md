# Chat UI - Interfaz para Ollama con Next.js

Una interfaz de chat moderna y elegante construida con Next.js 14, TypeScript y Tailwind CSS, integrada con Ollama para ejecutar modelos de lenguaje localmente.

## ğŸš€ CaracterÃ­sticas

- âœ¨ Interfaz moderna similar a ChatGPT
- ğŸ¦™ IntegraciÃ³n con Ollama para ejecutar modelos localmente
- ğŸ”„ Selector de modelos dinÃ¡mico
- ğŸ’¬ Chat en tiempo real
- ğŸ¨ DiseÃ±o responsive con Tailwind CSS
- ğŸŒ™ Soporte para modo oscuro
- âš¡ Server-side rendering con Next.js 14
- ğŸ“± Optimizado para dispositivos mÃ³viles
- ğŸ”„ Renderizado de Markdown en las respuestas

## ğŸ“‹ Requisitos previos

- Node.js 18+ 
- Ollama instalado y ejecutÃ¡ndose

## ğŸ¦™ InstalaciÃ³n de Ollama

Primero, instala Ollama desde [ollama.ai](https://ollama.ai):

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

Luego, descarga un modelo:
```bash
ollama pull llama2
# o cualquier otro modelo como:
# ollama pull mistral
# ollama pull codellama
# ollama pull llama3
```

Inicia el servidor de Ollama:
```bash
ollama serve
```

## ğŸ› ï¸ InstalaciÃ³n

1. Instala las dependencias:

```bash
npm install
```

2. (Opcional) Crea un archivo `.env.local` si quieres personalizar la URL de Ollama:

```bash
cp .env.local.example .env.local
```

Por defecto, la aplicaciÃ³n se conecta a `http://localhost:11434`.

## ï¿½ InstalaciÃ³n automÃ¡tica con script

Para una instalaciÃ³n rÃ¡pida en servidor Linux, puedes usar el script de instalaciÃ³n automatizado:

```bash
# Descargar o clonar el proyecto
git clone https://github.com/tu-usuario/uiollama.git
cd uiollama

# Dar permisos de ejecuciÃ³n al script
chmod +x install.sh

# Ejecutar el script de instalaciÃ³n
./install.sh
```

El script instalarÃ¡ y configurarÃ¡ automÃ¡ticamente:
- âœ… Node.js 18+ (si no estÃ¡ instalado)
- âœ… Dependencias del proyecto
- âœ… Build de producciÃ³n
- âœ… PM2 para gestiÃ³n de procesos
- âœ… Nginx como reverse proxy (opcional)
- âœ… Certificado SSL con Let's Encrypt (opcional)
- âœ… Firewall UFW (opcional)

**Nota:** El script NO instala Ollama. Debes instalarlo manualmente antes:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama3.3  # o el modelo que prefieras
```

## ï¿½ğŸš¦ Uso

### Modo desarrollo

```bash
npm run dev
```

Abre [http://localhost:3000](http://localhost:3000) en tu navegador.

### ProducciÃ³n

```bash
npm run build
npm start
```

## ğŸ“‚ Estructura del proyecto

```
uiollama/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â””â”€â”€ route.ts      # API route para Ollama
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â””â”€â”€ route.ts      # API route para listar modelos
â”‚   â”œâ”€â”€ globals.css           # Estilos globales
â”‚   â”œâ”€â”€ layout.tsx            # Layout principal
â”‚   â””â”€â”€ page.tsx              # PÃ¡gina principal
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ChatInterface.tsx     # Componente principal del chat
â”‚   â”œâ”€â”€ ChatInput.tsx         # Input para mensajes
â”‚   â”œâ”€â”€ Header.tsx            # Header de la aplicaciÃ³n
â”‚   â”œâ”€â”€ MessageBubble.tsx     # Burbuja de mensaje individual
â”‚   â”œâ”€â”€ MessageList.tsx       # Lista de mensajes
â”‚   â””â”€â”€ ModelSelector.tsx     # Selector de modelos de Ollama
â”œâ”€â”€ types/
â”‚   â””â”€â”€ chat.ts               # Tipos TypeScript
â”œâ”€â”€ .env.local.example        # Ejemplo de variables de entorno
â”œâ”€â”€ next.config.js            # ConfiguraciÃ³n de Next.js
â”œâ”€â”€ package.json              # Dependencias del proyecto
â”œâ”€â”€ tailwind.config.ts        # ConfiguraciÃ³n de Tailwind CSS
â””â”€â”€ tsconfig.json             # ConfiguraciÃ³n de TypeScript
```

## ğŸ¨ CaracterÃ­sticas de la interfaz

- **Header**: Logo, tÃ­tulo, selector de modelos y botÃ³n para limpiar el chat
- **Selector de modelos**: Dropdown para cambiar entre los modelos de Ollama instalados
- **Lista de mensajes**: Muestra la conversaciÃ³n con scroll automÃ¡tico
- **Input de chat**: Campo de texto con soporte para mÃºltiples lÃ­neas (Shift + Enter)
- **Burbujas de mensaje**: DiseÃ±o diferenciado para usuario y asistente
- **Indicador de carga**: AnimaciÃ³n mientras se espera la respuesta
- **Renderizado Markdown**: Las respuestas del asistente soportan formato Markdown

## ğŸ”§ PersonalizaciÃ³n

### Cambiar el modelo predeterminado

Edita [components/ChatInterface.tsx](components/ChatInterface.tsx#L11) y cambia el modelo inicial:

```typescript
const [selectedModel, setSelectedModel] = useState("mistral"); // o "llama2", "codellama", etc.
```

### Configurar URL personalizada de Ollama

Si Ollama estÃ¡ ejecutÃ¡ndose en otra mÃ¡quina o puerto, crea `.env.local`:

```bash
OLLAMA_API_URL=http://tu-servidor:11434
```

### Modelos disponibles

Algunos modelos populares de Ollama:
- `llama2` - Meta Llama 2 (7B)
- `llama3` - Meta Llama 3 (8B)
- `mistral` - Mistral 7B
- `codellama` - Code Llama (cÃ³digo)
- `phi` - Microsoft Phi-2 (2.7B)
- `gemma` - Google Gemma (2B/7B)

Consulta mÃ¡s modelos en: https://ollama.ai/library

## ï¿½ Despliegue en Servidor Linux

### Requisitos del servidor

- Ubuntu 20.04+ / Debian 11+ / CentOS 8+ o similar
- MÃ­nimo 4GB RAM (8GB+ recomendado para modelos grandes)
- Node.js 18+
- PM2 para gestiÃ³n de procesos
- Nginx (opcional, para reverse proxy)

### Paso 1: Actualizar el sistema

```bash
sudo apt update && sudo apt upgrade -y
```

### Paso 2: Instalar Node.js 18+

```bash
# Instalar Node.js usando NodeSource
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt install -y nodejs

# Verificar instalaciÃ³n
node --version
npm --version
```

### Paso 3: Instalar Ollama

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Verificar instalaciÃ³n
ollama --version

# Descargar un modelo
ollama pull llama3.3
# o cualquier otro modelo que prefieras
```

### Paso 4: Configurar Ollama como servicio

Ollama se instala automÃ¡ticamente como servicio systemd. Verifica su estado:

```bash
sudo systemctl status ollama

# Si no estÃ¡ activo, iniciarlo
sudo systemctl start ollama
sudo systemctl enable ollama
```

### Paso 5: Clonar el proyecto

```bash
# Navegar al directorio deseado
cd /var/www

# Clonar el repositorio (o subir archivos via SCP/SFTP)
git clone https://github.com/tu-usuario/uiollama.git
cd uiollama

# O si subes archivos manualmente:
# scp -r ./uiollama usuario@servidor:/var/www/
```

### Paso 6: Instalar dependencias y construir

```bash
# Instalar dependencias
npm install

# Construir para producciÃ³n
npm run build
```

### Paso 7: Configurar variables de entorno (opcional)

```bash
# Si necesitas personalizar la URL de Ollama
nano .env.local

# Agregar:
# OLLAMA_API_URL=http://localhost:11434

# Guardar con Ctrl+X, luego Y, luego Enter
```

### Paso 8: Instalar PM2 para gestiÃ³n de procesos

```bash
# Instalar PM2 globalmente
sudo npm install -g pm2

# Iniciar la aplicaciÃ³n con PM2
pm2 start npm --name "uiollama" -- start

# Configurar PM2 para iniciar al arrancar el sistema
pm2 startup
pm2 save

# Verificar estado
pm2 status
pm2 logs uiollama
```

### Paso 9: Configurar Firewall

```bash
# Permitir trÃ¡fico en el puerto 3000
sudo ufw allow 3000/tcp

# O si usas Nginx (paso siguiente), solo permite 80 y 443
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw enable
```

### Paso 10: Configurar Nginx como Reverse Proxy (Recomendado)

```bash
# Instalar Nginx
sudo apt install -y nginx

# Crear configuraciÃ³n del sitio
sudo nano /etc/nginx/sites-available/uiollama

# Agregar esta configuraciÃ³n:
```

```nginx
server {
    listen 80;
    server_name tu-dominio.com;  # Cambiar por tu dominio o IP

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

```bash
# Habilitar el sitio
sudo ln -s /etc/nginx/sites-available/uiollama /etc/nginx/sites-enabled/

# Verificar configuraciÃ³n
sudo nginx -t

# Reiniciar Nginx
sudo systemctl restart nginx
sudo systemctl enable nginx
```

### Paso 11: Configurar SSL con Let's Encrypt (Opcional pero recomendado)

```bash
# Instalar Certbot
sudo apt install -y certbot python3-certbot-nginx

# Obtener certificado SSL
sudo certbot --nginx -d tu-dominio.com

# RenovaciÃ³n automÃ¡tica (Certbot la configura automÃ¡ticamente)
sudo certbot renew --dry-run
```

### Comandos Ãºtiles para gestiÃ³n

```bash
# Ver logs de la aplicaciÃ³n
pm2 logs uiollama

# Reiniciar aplicaciÃ³n
pm2 restart uiollama

# Detener aplicaciÃ³n
pm2 stop uiollama

# Ver estado de Ollama
sudo systemctl status ollama

# Ver modelos instalados
ollama list

# Reiniciar Nginx
sudo systemctl restart nginx

# Monitorear recursos
pm2 monit
htop
```


# InstalaciÃ³n rÃ¡pida con script
```bash
git clone https://github.com/falconsoft3d/uiollama.git
cd uiollama
chmod +x install.sh
./install.sh
```bash


### Actualizar la aplicaciÃ³n

```bash
cd /var/www/uiollama

# Detener la aplicaciÃ³n
pm2 stop uiollama

# Actualizar cÃ³digo
git pull origin main
# o subir nuevos archivos

# Instalar nuevas dependencias (si las hay)
npm install

# Reconstruir
npm run build

# Reiniciar
pm2 restart uiollama
```

### SoluciÃ³n de problemas

**Problema: La aplicaciÃ³n no se conecta a Ollama**
```bash
# Verificar que Ollama estÃ© corriendo
sudo systemctl status ollama

# Ver logs de Ollama
sudo journalctl -u ollama -f
```

**Problema: Error de memoria al usar modelos grandes**
```bash
# Aumentar memoria swap si es necesario
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

**Problema: PM2 no se inicia al arrancar**
```bash
# Reconfigurar PM2 startup
pm2 unstartup
pm2 startup
pm2 save
```

## ï¿½ğŸ“ Licencia

MIT

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue primero para discutir los cambios que te gustarÃ­a hacer.

## ğŸ“§ Contacto

Para preguntas o sugerencias, abre un issue en este repositorio.
